# 第8章 集成学习

## 个体与集成

### 概念

- 集成学习：又称为多分类器系统、基于委员会的学习等
- 个体学习器：由一个现有的学习算法从训练数据产生，又称为基学习器
- 集成学习器：分为同质集成与异质集成
  - 同质集成：个体学习器相同
  - 异质集成：个体学习器不同，此时不称作基学习器，而称作“组件学习器”

### 集成学习的优点

- 泛化能力更强

### 集成学习的要求

- 个体学习器要有一定的准确性，并且要有多样性

- 根据Hoeffding不等式，集成的错误率为

  ![image-20200815115821717](https://i.loli.net/2020/08/15/f2gmps9khw6tJ5o.png)

  随着个体分类器数目T的增大，集成的错误率将指数级下降，最终趋向于零

### 集成学习的关键假设及其带来的问题

- 关键假设：基学习器的误差相互独立，这个假设在现实任务中显然不成立，因为个体学习器是为解决同一个问题训练出来的
- 冲突：个体学习器的“准确性”和“多样性”本身就存在冲突，准确性很高之后，要增加多样性就需牺牲准确性

### 集成学习的分类

- 个体学习器之间存在强依赖关系，必须串行生成的序列化方法：boosting
- 个体学习器之间不存在强依赖关系，可同时生成的并行化方法：bagging和随机森林

## Boosting

### 工作机制

- 先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值T， 最终将这T个基学习器进行加权结合

### 算法

- ![image-20200817120724423](https://i.loli.net/2020/08/17/dHsDPXjSwtxgzVL.png)
- 此处可见最终输出是一个符号函数，而我们的目的是希望最终输出能够使得损失函数最小化
  - ![image-20200817122020196](https://i.loli.net/2020/08/17/X3IyTROGJdUEBWf.png)
- 经过验证可知道指数损失函数是原本0/1损失函数的一致替代损失函数

### 要求

- 基学习器能对特定的数据分布进行学习
- 对无法接受带权样本的基学习算法可以通过“重采样法”来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练

### 偏差-方差分析

- boosting主要关注降低偏差

## Bagging与随机森林

### bagging

#### 工作机制

- 对训练样本进行采样，产生出若干个不同的子集（建议采样子集间相互有交叠，避免学习的有效性不足），再从每个数据子集中训练出一个基学习器，然后对基学习器进行并行式的集成

#### 算法

- ![image-20200817172642945](https://i.loli.net/2020/08/17/eIX1UoY5lAP2kCx.png)

- 对分类任务使用简单投票法，对回归任务使用简单平均法

- 同样票数的情形，可以随机选择一个，或根据学习器投票的置信度来决定胜者

- 复杂度与训练一个学习器的复杂度同阶

- 由于采用了自助采样的方法，因此基学习器只使用了初始训练集中约63.2%的样本，剩余的部分可用作验证集来对泛化性能进行“包外估计”

  ![image-20200819100109261](https://i.loli.net/2020/08/19/C9IZGjakATe8o6P.png)

  泛化误差的包外估计为

  ![image-20200819100324083](https://i.loli.net/2020/08/19/aqKiHb7PYNrkSAG.png)

- 包外估计的其他用途
  - 在决策树算法中辅助剪枝
  - 估计决策树中各结点的后验概率以辅助对零训练样本节点的处理
  - 在神经网络算法中辅助早期停止的决策，减小过拟合的风险

#### 偏差-方差分析

- bagging主要关注降低方差
- 适用场景：易受样本扰动的决策树、神经网络学习器

### 随机森林（Random Forest，简称RF）

#### 与bagging的区别

- RF的基学习器固定为决策树
- 引入随机属性选择：传统决策树会选择当前结点属性集合中最优的属性，而在RF中，先从该节点的属性集合中随机选择一个包含k个属性的子集，再从这个子集中选择一个最优的属性用于划分，k 控制了随机性的引入程度;若令k = d， 则基决策树的构建与传统决策树相同;若令k = 1，则是随机选择一个属性用于划分; 一般情况下，推荐值k取d以2为底的对数
- bagging中多样性来源于样本扰动，RF中除了样本扰动还有属性扰动
- RF的起始性能较差，但最终的泛化误差较好
- 随机森林的训练效率要高于使用决策树的bagging算法，因为bagging的决策树每一个结点都需要考虑所有的属性，而随机森林只需要考虑一个属性子集

## 结合策略

### 优势

- 总体样本空间大，单个学习器空间涉及的范围小，可能有多个学习器可以达到同等的性能，此时单个学习器的泛化性能可能不足，多个学习器结合能够扩大涉及的样本范围，提高泛化性能
- 单个学习器容易陷入局部极小点，通过多学习器结合可降低陷入局部极小点的风险
- 单个学习器包含的知识可能不涵盖真实的情况，通过多个学习器结合可以扩大知识范围从而逼近真实的情况

### 结合方式

#### 平均法

- 针对数值型
- 分为简单平均和加权平均
- 简单平均即基学习器的权重相同，加权平均指不同的基学习器有不同的权重，这个权重可以通过学习得到
- 学习权重的缺点
  - 训练样本不充分或存在噪声，使得学习得到的权重不完全可靠
  - 需要学习的权重过多时，容易造成过拟合
- 个体学习器性能差距大时使用加权平均，性能接近时使用简单平均

#### 投票法

- 绝对多数投票法：某标记得票过半数，则预测为该标记，否则拒绝预测
  - 可靠性高
  - 可能会拒绝预测
- 相对多数投票法：预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个
  - 是绝对多数投票法的退化
  - 一定能给出预测结果
- 软投票与硬投票
  - 软投票：预测给出的是介乎0-1的概率（类概率）
  - 硬投票：预测给出的是准确的0,1标记（类标记）
  - 性能比较：
    - 基于类概率的结合往往要优于基于类标记的结合

#### 学习法

- 通过多步的学习器进行结合
- stacking算法
  - ![image-20200821100948564](https://i.loli.net/2020/08/21/jqEzgAIvMl6dCLZ.png)
  - 为避免过拟合，一般用训练初级学习器未使用的样本来产生次级学习器的训练样本
  - 将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归(Multi-response Linear Regression，简称MLR) 作为次级学习算法效果较好
- 贝叶斯模型平均(Bayes Model Averaging，简称BMA)算法
  - 基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现
  - 前提：数据生成模型在当前考虑的模型中，且数据噪声很少
  - 通常性能不如Stacking，鲁棒性差，对模型近似误差敏感

## 多样性

### 误差分歧分解

- 集成的分歧为个体学习器误差的加权均值与集成学习器的误差均值的差

  ![image-20200826100140103](https://i.loli.net/2020/08/26/AqygYXSrwLUBCzj.png)

- 集成的泛化误差

  ![image-20200827115931696](https://i.loli.net/2020/08/27/FhzGUfIDeS1oLcN.png)

- 根据下式可见，个体学习器的准确性越高，多样性越好，则集成的效果越好

  ![image-20200827120321216](https://i.loli.net/2020/08/27/trYXPIdN4FLCibq.png)

### 多样性度量

- 用于度量集成中个体分类器的多样性

- 结果列联表

  ![image-20200827130707801](https://i.loli.net/2020/08/27/OJZcYCrTfyen93w.png)

- 衡量指标

  - ![image-20200827131016690](https://i.loli.net/2020/08/27/ck2pqFRgbOtjsUG.png)

    若两分类器在数据集上完全一致，则k=1，若是偶然达成一致，则k=0。k通常为非负数

    ![image-20200827132610062](https://i.loli.net/2020/08/27/xiQIvBgfuVCdZmh.png)

### 多样性增强

- 增强多样性的方法：对数据样本、输入属性、输出表示、算法参数进行扰动

- 数据样本扰动

  - 针对“不稳定基学习器”如决策树，神经网络有效
  - 针对“稳定基学习器”如线性学习器，支持向量机，朴素贝叶斯，k近邻学习器等效果不好

- 输入属性扰动

  - 从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器

  - 优点

    - 提高多样性
    - 减少冗余属性，提高训练速度

  - 缺点

    - 属性少或冗余属性比例低时不适用

  - 算法

    ![image-20200829122932381](https://i.loli.net/2020/08/29/xtGRiN4sCFVPXLc.png)

- 输出表示扰动

  - 对训练样本的类标记进行变动
  - 将分类输出转化为回归输出，再利用回归输出训练个体学习器
  - 将原任务拆解为多个可同时求解的子任务，如将多分类任务拆解成一系列的二分类任务

- 算法参数扰动

  - 通过正则化项来强制个体神经网络使用不同的参数
  - 用类似方式替代学习过程中的某些环节，例如可将决策树使用的属性选择机制替换成其他的属性选择机制

## 课后作业

[机器学习-第八章答案（来源网络）](https://blog.csdn.net/icefire_tyh/article/details/52194771)

