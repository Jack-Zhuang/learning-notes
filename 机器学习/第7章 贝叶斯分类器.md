# 第7章 贝叶斯分类器

## 贝叶斯决策论

### 适用场合

- 所有相关概率都已知的理想情形

### 条件风险

- ![image-20200806100658471](https://i.loli.net/2020/08/06/zFc2x3R7BZ6fLt8.png)
- 单类损失=该类出现的后验概率*该错误出现带来的损失
- 总损失=所有单类损失的求和

### 思路

- 通过最小化每个样本的条件风险从而达到总体风险的最小化
- 当目标为最小化分类错误率时，转化为使每一类的后验概率最大

### 问题

- 欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率，而这在现实任务中通常难以直接获得，因此问题转化为如何利用有限的样本集尽可能准确地估计出后验概率
- 两种思路
  - 判别式模型
    - 直接建模来预测分类的后验概率
  - 生成式模型
    - 根据现有数据的联合概率分布建模，再据此得到分类的后验概率
    - ![image-20200806104247127](https://i.loli.net/2020/08/06/wviIRfxudbGFakt.png)
    - 先验概率
      - 标签出现的概率，当训练集包含充足的独立同分布样本时，先验概率可以通过各类样本出现的频率进行估计
      - 似然是涉及到关于样本所有属性的联合概率，因此根据样本出现的频率来估计会出现问题，因为在训练集中未观测到的样本不代表不存在（出现概率为0），很多样本的属性取值在训练集中没有出现，因此无法直接根据样本集的出现频率来估计

## 极大似然估计（MLE）

### 参数估计

- ![image-20200806115114394](https://i.loli.net/2020/08/06/4NH2t9FnVwjCuYp.png)
- 频率主义学派
  - 认为参数虽然未知，但却是客观存在的固定值，可通过优化似然函数等的准则来确定参数值
- 贝叶斯学派
  - 认为参数是未观察到的随机变量，其本身也可有分布，可假定参数服从一个先验分布，基于观测到的数据来计算参数的后验分布

### 极大似然估计

- 参数对数据集的似然，极大似然估计就是寻找能最大化似然的参数值

  ![image-20200806223820158](https://i.loli.net/2020/08/06/jHubrI4UQpx2yAz.png)

- 为避免连乘操作造成下溢，通常使用对数似然

  ![image-20200806224245021](https://i.loli.net/2020/08/06/bXpYdxr1h3olWRy.png)

- ![image-20200806232618950](https://i.loli.net/2020/08/06/hRzJaOIZfbv325y.png)

## 朴素贝叶斯分类器

### 主要困难

- 类条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计而得

### 属性条件独立性假设

- 对已知类别，假设所有属性相互独立

- 基于上述假设可以将后验概率表示为

  ![image-20200807104442851](https://i.loli.net/2020/08/07/LIBubUVZRzmTMDX.png)

- 对离散属性

  ![image-20200807105418278](https://i.loli.net/2020/08/07/H5cBETCgxS13Vo6.png)

- 对连续属性，可使用概率密度函数

  ![image-20200807105601058](https://i.loli.net/2020/08/07/CmwxIoU3WDucFOk.png)

### 假如某个属性在训练集中没有与某个类同时出现过？

- 平滑处理

  - 拉普拉斯修正

    ![image-20200807112618524](https://i.loli.net/2020/08/07/rXkd9iGmgOTCDsK.png)

    - 作用：避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验(prior) 的影响也会逐渐变得可忽略，使得估值渐趋向于实际概率值.

- 朴素贝叶斯的多种使用方法

  - 数据集更替不频繁
    - 将所有的概率估值计算好储存起来，后续进行查表操作即可
  - 数据集更替频繁
    - 采用懒惰学习方法，先不进行任何训练，收到预测请求时再根据当前的数据集进行概率估值
  - 数据集不断增长
    - 在现有估值的基础上，仅对新增样本的属性值所设计的概率估值进行计数修正，从而实现增量学习

## 半朴素贝叶斯分类器

### 独依赖估计（one-dependent estimator）

- 假设每个属性在类别外最多仅依赖于一个其他属性

  ![image-20200808111138494](https://i.loli.net/2020/08/08/pbKhy8kJ6RLAzWZ.png)

- 常见做法

  - SPODE（super parent ODE方法）：假设所有的属性都依赖于同一个属性

    ![image-20200808111311066](https://i.loli.net/2020/08/08/NzR6eTxFnVyftva.png)

  - TAN（Tree Augmented naïve Bayes）

    - 步骤

      - 计算任何两个属性之间的条件互信息

        ![image-20200808103241390](https://i.loli.net/2020/08/08/q5coUk7vWl1FeRP.png)

      - 以属性为结点构建完全图，任意两个结点之间边的权重设为两节点的条件互信息

      - 构建此完全图的最大带权生成树，挑选根变量，将边置为有向

      - 加入类别结点y，增加从y到每个属性的有向边

        ![image-20200808111325402](https://i.loli.net/2020/08/08/aDr68EGtjTugI37.png)

    - 优点

      - 刻画了属性在已知类别情况下的相关性，保留了强相关属性之间的依赖性

  - AODE（Averaged One-Dependent Estimator）

    - 与SPODE的区别

      - 尝试将每个属性作为超父来构建SPODE，将具有足够训练数据支撑的SPODE集成起来作为最终结果

        ![image-20200808111427457](https://i.loli.net/2020/08/08/4VfJuBLgpNOTnje.png)

    - 与朴素贝叶斯分类器类似，AODE的训练过程也是“计数”，且无需模型选择，支持懒惰学习和增量学习

  - 能否进一步放松独依赖假设，使其成为k依赖假设？

    - 随着k的增加，所需要的训练样本数量将以指数级增加，若训练数据十分充分泛化性能可能得到提升，但在有限样本条件下，则又陷入估计高阶联合概率的泥沼

## 贝叶斯网络

### 结构

- 有向无环图

  - 刻画属性之间的依赖关系
  - 每个结点对应于一个属性

- 条件概率表

  - 描述属性的联合概率分布

- 优点

  - 表达了属性间的条件独立性

- 假设

  - 给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立

    ![image-20200810112704541](https://i.loli.net/2020/08/10/dn2CozUZfXYANqw.png)

    ![image-20200810112728330](https://i.loli.net/2020/08/10/bk6ijsaoI2xuNL4.png)

- 三种典型依赖关系

  ![image-20200810112824406](https://i.loli.net/2020/08/10/2La7F53V8MtqYZr.png)

  - 同父结构中，给定父结点的取值，则子结点条件独立
  - 顺序结构中，给定x结点的值，则y结点与z结点条件独立
  - V型结构中，给定子结点的值，两父结点必不独立，当子结点的值完全未知时，两父结点是相互独立的（又称为边际独立性）

- 有向分离

  - 用于分析有向图中变量间的条件独立性
  - 步骤
    - 找出有向图中的所有V型结构，在V 型结构的两个父结点之间加上一条无向边（moralization）
    - 将所有有向边改为无向边

### 学习

- 步骤
  - 根据训练数据集找出贝叶斯网结构
    - 评分搜索方法：常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习目标是找到一个能以最短编码长度描述训练数据的模型，此时编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度
  - 根据训练数据计算在该贝叶斯网中的概率分布
  - 选择综合编码长度（包括描述网络和编码数据）最短的贝叶斯网（最小描述长度准则，Minimal Description Length，简称MDL）
  - 给定训练集D，贝叶斯网的评分函数可写为
  - ![image-20200811163856770](https://i.loli.net/2020/08/11/sBeGkg6ilCAhycE.png)
  - ![image-20200811164810916](https://i.loli.net/2020/08/11/69lhJdm3WQqEXCA.png)
  - ![image-20200811165622717](https://i.loli.net/2020/08/11/v5D6pf3EzCXZbkY.png)
- 搜索难题：从所有可能的网络空间中搜索最优贝叶斯网结构是一个NP难问题
  - 常用策略
    - 贪心法：从某个网络结构出发，每次调整一条边（增加、删除或调整方向），直到评分函数值不再降低为止
    - 网络结构约束：通过约束网络结构来削减搜索空间

### 推断

- 理想情况：根据联合概率分布来精确计算后验概率（NP难）

- 近似推断：吉布斯采样

  - ![image-20200811174927547](https://i.loli.net/2020/08/11/fVpeMoJXcq13HwL.png)

  - 随机产生一个与证据一致的样本作为起始点

  - 对非证据变量逐个进行采样改变其取值，采样概率根据贝叶斯网以及其他变量的当前取值计算获得

  - 经过T次采样后得到的与待查询变量一致的样本个数，则可近似估计出后验概率

    ![image-20200811175354261](https://i.loli.net/2020/08/11/eRjiTmQdVMXHsb6.png)

  - 本质上是一种马尔科夫链
  - ![image-20200811182511391](https://i.loli.net/2020/08/11/hxewtI1BcNg6yai.png)
  - 缺点：
    - 马尔科夫链需要长时间才能趋于平稳分布，因此吉布斯采样算法的收敛速度较慢
    - 如贝叶斯网中存在极端概率“0”，“1”则不能保证马尔科夫链存在平稳分布

## EM算法

### 训练样本不完整

- 存在未观测的变量

  ![image-20200811232102579](https://i.loli.net/2020/08/11/7NUaKl8PqBXijOF.png)

  - 由于Z是隐变量，我们通过对Z计算期望，来最大化已观测数据的对数“边际似然”
  - ![image-20200811232541682](https://i.loli.net/2020/08/11/wh85sPWMn2AiQUJ.png)
  - ![image-20200811232830030](https://i.loli.net/2020/08/11/4t25DrYOPEfjakw.png)

## 课后作业

[机器学习（周志华）课后习题——第七章——贝叶斯分类](https://zhuanlan.zhihu.com/p/51768750)





