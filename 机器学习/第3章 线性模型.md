# 第3章 线性模型

## 基本形式

- 目的：学的一个个通过属性的线性组合来进行预测的函数
- 公式：![](https://latex.codecogs.com/gif.latex?f(\boldsymbol{x})=w_{1}&space;x_{1}&plus;w_{2}&space;x_{2}&plus;\ldots&plus;w_{d}&space;x_{d}&plus;b)或 ![](https://latex.codecogs.com/gif.latex?f(x)=w^{\mathrm{T}}&space;x&plus;b)

- 线性模型的优势：
  - 可解释性强

## 线性回归

- 性能度量
  
  - 均方误差，几何意义为欧氏距离
  
- 求解方法
  - 基于均方误差最小化：最小二乘法
  - 最小化误差![](https://latex.codecogs.com/gif.latex?E_{(w,&space;b)}=\sum_{i=1}^{m}\left(y_{i}-w&space;x_{i}-b\right)^{2})
  - 求误差函数关于w，b的导数![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Barray%7D%7Bl%7D%20%5Cfrac%7B%5Cpartial%20E_%7B%28w%2C%20b%29%7D%7D%7B%5Cpartial%20w%7D%3D2%5Cleft%28w%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20x_%7Bi%7D%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Cleft%28y_%7Bi%7D-b%5Cright%29%20x_%7Bi%7D%5Cright%29%20%5C%5C%20%5Cfrac%7B%5Cpartial%20E_%7B%28w%2C%20b%29%7D%7D%7B%5Cpartial%20b%7D%3D2%5Cleft%28m%20b-%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Cleft%28y_%7Bi%7D-w%20x_%7Bi%7D%5Cright%29%5Cright%29%20%5Cend%7Barray%7D)
  - 令导数为0，可求得![](https://latex.codecogs.com/gif.latex?w=\frac{\sum_{i=1}^{m}&space;y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m}&space;x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m}&space;x_{i}\right)^{2}})， ![](https://latex.codecogs.com/gif.latex?b=\frac{1}{m}&space;\sum_{i=1}^{m}\left(y_{i}-w&space;x_{i}\right))
  - 针对多元线性回归
    - ![image-20200615214133538](https://i.loli.net/2020/06/15/D5uRdjhnkPOUgzG.png)
    - 损失函数为![](https://latex.codecogs.com/gif.latex?E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X}&space;\hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X}&space;\hat{\boldsymbol{w}}))
    - 对w求导可得![](https://latex.codecogs.com/gif.latex?\frac{\partial&space;E_{\hat{w}}}{\partial&space;\hat{w}}=2&space;\mathbf{X}^{\mathrm{T}}(\mathbf{X}&space;\hat{\boldsymbol{w}}-\boldsymbol{y}))，从而 ![](https://latex.codecogs.com/gif.latex?\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}}&space;\mathbf{X}\right)^{-1}&space;\mathbf{X}^{\mathrm{T}}&space;\boldsymbol{y})
    - 当![](https://latex.codecogs.com/gif.latex?\mathbf{x}^{\mathrm{T}}&space;\mathbf{x})为为满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时可解得 ![](https://latex.codecogs.com/gif.latex?\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}}&space;\mathbf{X}\right)^{-1}&space;\mathbf{X}^{\mathrm{T}}&space;\boldsymbol{y})
    - 现实任务中![](https://latex.codecogs.com/gif.latex?\mathbf{x}^{\mathrm{T}}&space;\mathbf{x})往往不是满秩矩阵，在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致X的列数多于行数，从而可以解得多个w，这些w都能使得均方误差最小化，此时选择哪个解作为输出将有学习算法的归纳偏好决定，常见的做法是引入正则化项
  
  ### 线性回归的实现
  
  ```python
  # 线性回归以及多元线性回归
  
  import numpy as np
  import matplotlib.pyplot as plt
  
  '''
  输入：维度dim, 生成点的个数counts
  输出：随机生成的数据点，数据点符合标准正态分布，固定了random的种子以保证每次结果的一致性（可以调整），最后一列是y值
  '''
  def gen_random_pts(dim,counts):
      np.random.seed(1)  # 设置random种子
      dataset = np.random.randn(counts,dim)  # 生成数据集
      print(f"dataset = {dataset}")
      return dataset
  
  '''
  输入：生成的二维数据集，列数为2，第一列为x值，第二列为y值
  输出：满足最小二乘法求解均方误差最小化的w与b值，可参考西瓜书公式（3.7）与（3.8）
  '''
  def linear_regression(dataset):
  #     print(dataset[:,0])
      x_avg = np.average(dataset[:,0])  # 求x平均值
      numerator = 0
      denominator_part1 = 0
      denominator_part2 = 0
      b = 0
      for i in range(dataset.shape[0]):
          numerator += dataset[i][1]*(dataset[i][0]-x_avg)
          denominator_part1 += dataset[i][0]**2
          denominator_part2 += dataset[i][0]
      denominator_part2 = (denominator_part2**2)/dataset.shape[0]
      denominator = denominator_part1 - denominator_part2
      w = numerator/denominator
      for i in range(dataset.shape[0]):
          b += dataset[i][1]-w*dataset[i][0]
      b = b/dataset.shape[0]
      return w,b
  
  
  '''
  输入：数据集，最后一列为y值，前面的列为一维或多维的x值
  输出：满足最小二乘法求解均方误差最小化的w与b值，可参考西瓜书公式（3.11）
  '''
  def mul_linear_regression(dataset):
      y = dataset[:,-1]
      dataset = dataset[:,0:dataset.shape[1]-1]
      dataset = np.append(dataset,np.ones([dataset.shape[0],1]),axis = 1)
  #     print(y)
  #     print(dataset)
      w = np.dot(np.dot(np.linalg.inv(np.dot(dataset.T,dataset)),dataset.T),y)
      return w
  
  
  '''
  输入：w，b以及数据集
  输出：数据集点以及拟合曲线的图像（仅适用于二维）
  '''
  def draw_graph(w,b,dataset):
      plt.figure()
      x = np.linspace(-3,3,30)
      y = w*x + b
      for i in dataset:
          plt.scatter(i[0],i[1],color = 'red')
      plt.plot(x,y,color = 'blue')
      plt.rcParams['font.sans-serif'] = ['SimHei']  # 保证中文标题可以使用
      plt.rcParams['axes.unicode_minus'] = False
      plt.title('二维情况的示意图')
      plt.show()
      
  def main():
      dataset = gen_random_pts(2,10)  # 第一个参数是维度，第二个参数是点数，生成结果中最后一列是y值
      w,b = linear_regression(dataset)  # 仅在二维情况下可用，如果多维度请将此条目注释掉
      print(f"\n(w_2dim,b_2dim) = {(w,b)}")
      draw_graph(w,b,dataset)  # 仅在二维情况下可用，如果多维度请将此条目注释掉
      w_mul = mul_linear_regression(dataset)
      print(f"(w_mul, b_mul) = {w_mul}")
      
  main()
  ```
  
  - 二维的求解结果示例
    - ![image-20200616233515769](https://i.loli.net/2020/06/16/jWFTDNc61f4RSLE.png)
  - 多维的求解结果示例
    - ![image-20200616234043438](https://i.loli.net/2020/06/16/nFmkaXshxy7MqAt.png)
  
  ```python
  # 使用sklearn完成线性回归
  
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.linear_model import LinearRegression
  
  # 构造数据集
  np.random.seed(1)
  dataset = np.random.randn(10,2)
  print(dataset)
  x = dataset[:,0].reshape(dataset.shape[0],1)
  y = dataset[:,1].reshape(dataset.shape[0],1)
  
  # 使用线性回归模型训练
  model = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
  model.fit(X, y)
  print(f"截距为：{model.intercept_}")
  print(f"系数为：{model.coef_}")
  
  # 预测
  y_predict = model.predict(X)
  
  # 画图
  plt.figure()
  plt.scatter(X, y, marker='x')
  plt.plot(X, y_predict,c='r')
  plt.show()
  ```
  
  - ![image-20200617110105851](https://i.loli.net/2020/06/17/8e4pqiAB13S5Nzj.png)
  - 参考资料：
    - [【机器学习+sklearn框架】（一） 线性模型之Linear Regression](https://blog.csdn.net/walk_power/article/details/82924363)
    - [sklearn API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
  
  ```python
  # 不调用API的线性回归实现
  
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  # 线性回归
  class LinearRegression():
      def __init__(self):
          self.w = 0
      def fit(self,x,y):
          x = np.insert(x, 0, 1, axis = 1)
          print(x.shape)        
          self.w = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)
      def predict(self,x):
          x = np.insert(x, 0, 1, axis = 1)
          y_predict = x.dot(self.w)
          return y_predict
      
  # 构造数据集
  np.random.seed(1)
  dataset = np.random.randn(10,2)
  print(dataset)
  x = dataset[:,0].reshape(dataset.shape[0],1)
  y = dataset[:,1].reshape(dataset.shape[0],1)
  
  # 使用线性回归模型训练
  model = LinearRegression()
  model.fit(X,y)
  print(f"截距为：{model.w[0]}")
  print(f"系数为：{model.w[1]}")
  
  # 预测
  y_predict = model.predict(x)
  
  # 画图
  plt.figure()
  plt.scatter(x, y, marker='x')
  plt.plot(x, y_predict,c='b')
  plt.show()
  
  # 单点预测
  print(f"如输入x = 5.36（注意数据结构）， 则输出为{s.predict(np.array([[5.36]]))}")
  ```
  
  - ![image-20200617110520175](https://i.loli.net/2020/06/17/j1nh5OdP3EliI9F.png)

## 对数几率回归（logistic regression）

- 是一种分类方法，目的是提供一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来，本质上是一个“广义线性模型”，即满足![](https://latex.codecogs.com/gif.latex?y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b\right))， 其中![](https://latex.codecogs.com/gif.latex?y=g(*))被称为“联系函数”
- 我们想要单位阶跃函数的性质，所以选择一个sigmoid函数![](https://latex.codecogs.com/gif.latex?y=\frac{1}{1&plus;e^{-z}})来作为替代函数
- 公式：![](https://latex.codecogs.com/gif.latex?y=\frac{1}{1&plus;e^{-\left(\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b\right)}})
- 优点：
  - 直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题
  - 可得到近似概率预测
  - 对率函数是任意阶可导
- 推导：
  - ![](https://latex.codecogs.com/gif.latex?\ln&space;\frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b)（1）
  - ![](https://latex.codecogs.com/gif.latex?\ln&space;\frac{p(y=1&space;|&space;\boldsymbol{x})}{p(y=0&space;|&space;\boldsymbol{x})}=\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b)（2），其中![](https://latex.codecogs.com/gif.latex?y=p(y=1&space;|&space;\boldsymbol{x}))，![](https://latex.codecogs.com/gif.latex?1-y=p(y=0&space;|&space;\boldsymbol{x}))
  - 利用公式（1）可以推出![](https://latex.codecogs.com/gif.latex?y=p(y=1&space;|&space;\boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b}}{1&plus;e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b}})，![](https://latex.codecogs.com/gif.latex?1-y=p(y=0&space;|&space;\boldsymbol{x})=\frac{1}{1&plus;e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x}&plus;b}})。
  - 当我们拥有m个数据点时，根据极大似然估计法，我们希望最大化![](https://latex.codecogs.com/gif.latex?Target=\prod_{i=1}^{m}&space;p_{i}^{y_{i}}(1-p_{i})^{1-y_{i}})
  - 观察形式，发现两边同时取对数时可以将累乘转化为求和，可以得到![](https://latex.codecogs.com/gif.latex?%5Cln%5Cleft%28Target%5Cright%29%3D%5Cln%5Cleft%28%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20p_%7Bi%7D%5E%7By_%7Bi%7D%7D%281-p_%7Bi%7D%29%5E%7B1-y_%7Bi%7D%7D%5Cright%29)
  - 即![](https://latex.codecogs.com/gif.latex?%5Cln%20%5Cleft%28Target%20%5Cright%20%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cln%20%5Cleft%28p_%7Bi%7D%5E%7By_%7Bi%7D%7D%281-p_%7Bi%7D%29%5E%7B1-y_%7Bi%7D%7D%5Cright%29)，注：此处的![](https://latex.codecogs.com/gif.latex?p_{i})即上述公式（1）中的y（预测标签），![](https://latex.codecogs.com/gif.latex?y_{i})是数据点的真实标签
  - 根据公式（1），再根据对数运算法则有
  - ![](https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\ln&space;\left(Target&space;\right&space;)&=\sum_{i=1}^{m}\left[y_{i}\ln&space;\left(p_{i}&space;\right&space;)&plus;\left(1-y_{i}&space;\right&space;)\ln&space;\left(1-p_{i}&space;\right&space;)&space;\right&space;]\\&space;&=\sum_{i=1}^{m}\left[y_{i}\ln&space;\left(\frac{p_{i}}{1-p_{i}}&space;\right&space;)&plus;\ln&space;\left(1-p_{i}&space;\right&space;)&space;\right&space;]\\&space;&=\sum_{i=1}^{m}\left[y_{i}\left(\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b&space;\right&space;)&plus;&space;\ln&space;\left(\frac{1}{1&plus;e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b}}&space;\right&space;)\right&space;]\\&space;&=\sum_{i=1}^{m}\left[y_{i}\left(\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b&space;\right&space;)-\ln&space;\left(1&plus;e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b}&space;\right&space;)&space;\right&space;]&space;\end{aligned}) 
  - 最大化上式结果等价于最小化其相反数因此可得
  - ![](https://latex.codecogs.com/gif.latex?\begin{aligned}&space;Loss\left(\boldsymbol{w};\boldsymbol{b}&space;\right&space;)&space;&=-\ln&space;\left(Target&space;\right&space;)\\&space;&=-\sum_{i=1}^{m}\left[y_{i}\left(\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b&space;\right&space;)-\ln&space;\left(1&plus;e^{\boldsymbol{w}^{\mathrm{T}}&space;\boldsymbol{x_{i}}&plus;b}&space;\right&space;)&space;\right&space;]&space;&space;\end{aligned})
  - 可得梯度为
  - ![](https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;Loss\left(\boldsymbol{w};\boldsymbol{b}&space;\right&space;)}{\partial&space;\boldsymbol{w}}&space;&=-\sum_{i=1}^{m}\left[y_{i}\boldsymbol{x_{i}}-p(y=1&space;|&space;\boldsymbol{x_{i}})\boldsymbol{x_{i}}&space;\right&space;]\\&space;&=\sum_{i=1}^{m}\left[p(y=1&space;|&space;\boldsymbol{x_{i}}\right)&space;-y_{i}&space;]\boldsymbol{x_{i}}&space;\end{aligned})
  - 参考资料：
    - [逻辑回归 logistics regression 公式推导](https://zhuanlan.zhihu.com/p/44591359)
    - [南瓜书PumpkinBook第三章公式推导](https://datawhalechina.github.io/pumpkin-book/#/chapter3/chapter3?id=_310)

### 对数几率回归的代码实现







