# 第3章 线性模型

## 基本形式

- 目的：学的一个个通过属性的线性组合来进行预测的函数
- 公式：![](https://latex.codecogs.com/gif.latex?f(\boldsymbol{x})=w_{1}&space;x_{1}&plus;w_{2}&space;x_{2}&plus;\ldots&plus;w_{d}&space;x_{d}&plus;b)或 ![](https://latex.codecogs.com/gif.latex?f(x)=w^{\mathrm{T}}&space;x&plus;b)

- 线性模型的优势：
  - 可解释性强

## 线性回归

- 性能度量
  
  - 均方误差，几何意义为欧氏距离
  
- 求解方法
  - 基于均方误差最小化：最小二乘法
  - 最小化误差![](https://latex.codecogs.com/gif.latex?E_{(w,&space;b)}=\sum_{i=1}^{m}\left(y_{i}-w&space;x_{i}-b\right)^{2})
  - 求误差函数关于w，b的导数![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Barray%7D%7Bl%7D%20%5Cfrac%7B%5Cpartial%20E_%7B%28w%2C%20b%29%7D%7D%7B%5Cpartial%20w%7D%3D2%5Cleft%28w%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20x_%7Bi%7D%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Cleft%28y_%7Bi%7D-b%5Cright%29%20x_%7Bi%7D%5Cright%29%20%5C%5C%20%5Cfrac%7B%5Cpartial%20E_%7B%28w%2C%20b%29%7D%7D%7B%5Cpartial%20b%7D%3D2%5Cleft%28m%20b-%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Cleft%28y_%7Bi%7D-w%20x_%7Bi%7D%5Cright%29%5Cright%29%20%5Cend%7Barray%7D)
  - 令导数为0，可求得![](https://latex.codecogs.com/gif.latex?w=\frac{\sum_{i=1}^{m}&space;y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m}&space;x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m}&space;x_{i}\right)^{2}})， ![](https://latex.codecogs.com/gif.latex?b=\frac{1}{m}&space;\sum_{i=1}^{m}\left(y_{i}-w&space;x_{i}\right))
  - 针对多元线性回归
    - ![image-20200615214133538](https://i.loli.net/2020/06/15/D5uRdjhnkPOUgzG.png)
    - 损失函数为![](https://latex.codecogs.com/gif.latex?E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X}&space;\hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X}&space;\hat{\boldsymbol{w}}))
    - 对w求导可得![](https://latex.codecogs.com/gif.latex?\frac{\partial&space;E_{\hat{w}}}{\partial&space;\hat{w}}=2&space;\mathbf{X}^{\mathrm{T}}(\mathbf{X}&space;\hat{\boldsymbol{w}}-\boldsymbol{y}))，从而 ![](https://latex.codecogs.com/gif.latex?\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}}&space;\mathbf{X}\right)^{-1}&space;\mathbf{X}^{\mathrm{T}}&space;\boldsymbol{y})
    - 当![](https://latex.codecogs.com/gif.latex?\mathbf{x}^{\mathrm{T}}&space;\mathbf{x})为为满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时可解得 ![](https://latex.codecogs.com/gif.latex?\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}}&space;\mathbf{X}\right)^{-1}&space;\mathbf{X}^{\mathrm{T}}&space;\boldsymbol{y})
    - 现实任务中![](https://latex.codecogs.com/gif.latex?\mathbf{x}^{\mathrm{T}}&space;\mathbf{x})往往不是满秩矩阵，在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致X的列数多于行数，从而可以解得多个w，这些w都能使得均方误差最小化，此时选择哪个解作为输出将有学习算法的归纳偏好决定，常见的做法是引入正则化项
  
  ## 部分代码实现
  
  ```python
  # 线性回归以及多元线性回归
  
  import numpy as np
  import matplotlib.pyplot as plt
  
  '''
  输入：维度dim, 生成点的个数counts
  输出：随机生成的数据点，数据点符合标准正态分布，固定了random的种子以保证每次结果的一致性（可以调整），最后一列是y值
  '''
  def gen_random_pts(dim,counts):
      np.random.seed(1)  # 设置random种子
      dataset = np.random.randn(counts,dim)  # 生成数据集
      print(f"dataset = {dataset}")
      return dataset
  
  '''
  输入：生成的二维数据集，列数为2，第一列为x值，第二列为y值
  输出：满足最小二乘法求解均方误差最小化的w与b值，可参考西瓜书公式（3.7）与（3.8）
  '''
  def linear_regression(dataset):
  #     print(dataset[:,0])
      x_avg = np.average(dataset[:,0])  # 求x平均值
      numerator = 0
      denominator_part1 = 0
      denominator_part2 = 0
      b = 0
      for i in range(dataset.shape[0]):
          numerator += dataset[i][1]*(dataset[i][0]-x_avg)
          denominator_part1 += dataset[i][0]**2
          denominator_part2 += dataset[i][0]
      denominator_part2 = (denominator_part2**2)/dataset.shape[0]
      denominator = denominator_part1 - denominator_part2
      w = numerator/denominator
      for i in range(dataset.shape[0]):
          b += dataset[i][1]-w*dataset[i][0]
      b = b/dataset.shape[0]
      return w,b
  
  
  '''
  输入：数据集，最后一列为y值，前面的列为一维或多维的x值
  输出：满足最小二乘法求解均方误差最小化的w与b值，可参考西瓜书公式（3.11）
  '''
  def mul_linear_regression(dataset):
      y = dataset[:,-1]
      dataset = dataset[:,0:dataset.shape[1]-1]
      dataset = np.append(dataset,np.ones([dataset.shape[0],1]),axis = 1)
  #     print(y)
  #     print(dataset)
      w = np.dot(np.dot(np.linalg.inv(np.dot(dataset.T,dataset)),dataset.T),y)
      return w
  
  
  '''
  输入：w，b以及数据集
  输出：数据集点以及拟合曲线的图像（仅适用于二维）
  '''
  def draw_graph(w,b,dataset):
      plt.figure()
      x = np.linspace(-3,3,30)
      y = w*x + b
      for i in dataset:
          plt.scatter(i[0],i[1],color = 'red')
      plt.plot(x,y,color = 'blue')
      plt.rcParams['font.sans-serif'] = ['SimHei']  # 保证中文标题可以使用
      plt.rcParams['axes.unicode_minus'] = False
      plt.title('二维情况的示意图')
      plt.show()
      
  def main():
      dataset = gen_random_pts(2,10)  # 第一个参数是维度，第二个参数是点数，生成结果中最后一列是y值
      w,b = linear_regression(dataset)  # 仅在二维情况下可用，如果多维度请将此条目注释掉
      print(f"\n(w_2dim,b_2dim) = {(w,b)}")
      draw_graph(w,b,dataset)  # 仅在二维情况下可用，如果多维度请将此条目注释掉
      w_mul = mul_linear_regression(dataset)
      print(f"(w_mul, b_mul) = {w_mul}")
      
  main()
  ```
  
  - 二维的求解结果示例
    - ![image-20200616233515769](https://i.loli.net/2020/06/16/jWFTDNc61f4RSLE.png)
  - 多维的求解结果示例
    - ![image-20200616234043438](https://i.loli.net/2020/06/16/nFmkaXshxy7MqAt.png)