# 第4章 决策树

## 基本流程

- 结点递归返回的条件
  - 当前结点包含的样本全属于同一类别，无需划分
  - 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
    - 把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别，利用当前结点的后验分布
  - 当前结点包含的样本集合为空，不能划分
    - 把当前结点标记为叶结点，且将其类别设定为其父结点所含样本最多的类别，利用父结点的样本分布作为当前结点的先验分布

## 划分选择

- 信息增益

  - 信息熵：D为样本集合，p_k为集合D中第k类样本所占的比例，熵值越小，集合D的纯度越高

    ![](https://latex.codecogs.com/gif.latex?\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}&space;p_{k}&space;\log&space;_{2}&space;p_{k})

  - 信息增益：根据选定的属性将集合D进行分割，分别求取分割以后各个子集信息熵，在通过**加权求和**得到本次分割得到的信息熵，用原信息熵与分割后的信息熵做差，得到的差值即为信息增益，这也**是ID3决策树的划分准则**

    ![](https://latex.codecogs.com/gif.latex?\operatorname{Gain}(D,&space;a)=\operatorname{Ent}(D)-\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\operatorname{Ent}\left(D^{v}\right))

    - 信息增益越大，意味着使用该属性进行划分得到的”纯度提升“越大
    - 对可取值数目较多的属性有所偏好，因此出现了使用信息增益率作为判断依据的C4.5算法

  - 信息增益率：增益率定义为

    ![](https://latex.codecogs.com/gif.latex?\text&space;{&space;Gain&space;ratio&space;}(D,&space;a)=\frac{\operatorname{Gain}(D,&space;a)}{\mathrm{IV}(a)})

    其中

    ![](https://latex.codecogs.com/gif.latex?\mathrm{IV}(a)=-\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\log&space;_{2}&space;\frac{\left|D^{v}\right|}{|D|})

    称为属性“a”的固有值

    - 属性值的取值数目越多，固有值越大
    - 固有值越大，则增益率越小
    - 增益率偏好可取值数目少的属性，因此C4.5算法**先找出信息增益高于平均水平的属性，再筛选其中增益率最高的属性作为划分属性**

  - 基尼指数

    - 是CART（Classification And Regression Tree）算法的划分属性

    - 其公式为

      ![](https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\operatorname{Gini}(D)&space;&=\sum_{k=1}^{|\mathcal{Y}|}&space;\sum_{k^{\prime}&space;\neq&space;k}&space;p_{k}&space;p_{k^{\prime}}&space;\\&space;&=1-\sum_{k=1}^{|\mathcal{Y}|}&space;p_{k}^{2}&space;\end{aligned})

    - 按照属性a分割得到的基尼指数是每个子集中基尼指数值的加权求和值，公式如下

      ![](https://latex.codecogs.com/gif.latex?\text&space;{&space;Gini&space;}&space;\operatorname{index}(D,&space;a)=\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\operatorname{Gini}\left(D^{v}\right))

    - 划分后能使得基尼指数最小的属性则为最优划分属性

## 剪枝处理

- 目的：应付过拟合（分支过多的情况）
- 剪枝策略
  - 预剪枝
    - 指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
    - 一棒子打死的方案，有些分支虽然当前的泛化性能可能下降，但后续的划分可能带来提升，而预剪枝无法处理这种情况，所以有机会导致欠拟合的情况
  - 后剪枝
    - 是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点
- 判断泛化性能的依据
  - 使用第二章中的性能评估方法：留出法等



