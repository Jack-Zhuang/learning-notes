# 第4章 决策树

## 基本流程

- 结点递归返回的条件
  - 当前结点包含的样本全属于同一类别，无需划分
  - 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
    - 把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别，利用当前结点的后验分布
  - 当前结点包含的样本集合为空，不能划分
    - 把当前结点标记为叶结点，且将其类别设定为其父结点所含样本最多的类别，利用父结点的样本分布作为当前结点的先验分布

## 划分选择

- 信息增益

  - 信息熵：D为样本集合，p_k为集合D中第k类样本所占的比例，熵值越小，集合D的纯度越高

    ![](https://latex.codecogs.com/gif.latex?\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}&space;p_{k}&space;\log&space;_{2}&space;p_{k})

  - 信息增益：根据选定的属性将集合D进行分割，分别求取分割以后各个子集信息熵，在通过**加权求和**得到本次分割得到的信息熵，用原信息熵与分割后的信息熵做差，得到的差值即为信息增益，这也**是ID3决策树的划分准则**

    ![](https://latex.codecogs.com/gif.latex?\operatorname{Gain}(D,&space;a)=\operatorname{Ent}(D)-\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\operatorname{Ent}\left(D^{v}\right))

    - 信息增益越大，意味着使用该属性进行划分得到的”纯度提升“越大
    - 对可取值数目较多的属性有所偏好，因此出现了使用信息增益率作为判断依据的C4.5算法

  - 信息增益率：增益率定义为

    ![](https://latex.codecogs.com/gif.latex?\text&space;{&space;Gain&space;ratio&space;}(D,&space;a)=\frac{\operatorname{Gain}(D,&space;a)}{\mathrm{IV}(a)})

    其中

    ![](https://latex.codecogs.com/gif.latex?\mathrm{IV}(a)=-\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\log&space;_{2}&space;\frac{\left|D^{v}\right|}{|D|})

    称为属性“a”的固有值

    - 属性值的取值数目越多，固有值越大
    - 固有值越大，则增益率越小
    - 增益率偏好可取值数目少的属性，因此C4.5算法**先找出信息增益高于平均水平的属性，再筛选其中增益率最高的属性作为划分属性**

  - 基尼指数

    - 是CART（Classification And Regression Tree）算法的划分属性

    - 其公式为

      ![](https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\operatorname{Gini}(D)&space;&=\sum_{k=1}^{|\mathcal{Y}|}&space;\sum_{k^{\prime}&space;\neq&space;k}&space;p_{k}&space;p_{k^{\prime}}&space;\\&space;&=1-\sum_{k=1}^{|\mathcal{Y}|}&space;p_{k}^{2}&space;\end{aligned})

    - 按照属性a分割得到的基尼指数是每个子集中基尼指数值的加权求和值，公式如下

      ![](https://latex.codecogs.com/gif.latex?\text&space;{&space;Gini&space;}&space;\operatorname{index}(D,&space;a)=\sum_{v=1}^{V}&space;\frac{\left|D^{v}\right|}{|D|}&space;\operatorname{Gini}\left(D^{v}\right))

    - 划分后能使得基尼指数最小的属性则为最优划分属性

## 剪枝处理

- 目的：应付过拟合（分支过多的情况）
- 剪枝策略
  - 预剪枝
    - 指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
    - 一棒子打死的方案，有些分支虽然当前的泛化性能可能下降，但后续的划分可能带来提升，而预剪枝无法处理这种情况，所以有机会导致欠拟合的情况
    - 优点：训练耗时短
    - 缺点：容易导致欠拟合
  - 后剪枝
    - 是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点（如劣化或同等效果则不进行剪枝）
    - 优点：泛化性能强，欠拟合风险小
    - 缺点：训练耗时长
  - 一些结论
    - 后剪枝决策树通常比预剪枝决策树保留了更多的分支，因为预剪枝是基于贪心策略进行剪枝的，会损失一部分提升泛化性能的机会，后剪枝策略的欠拟合风险小，泛化性能更优秀，但后剪枝需要先建立完整的二叉树，且从底向上逐一进行考察，其训练时间开销会远大于预剪枝决策树
- 判断泛化性能的依据
  - 使用第二章中的性能评估方法：留出法等

## 连续与缺失值

- 连续值的分割方法

  - 二分法（用于C4.5决策树）
    - 可以通过信息增益的方法来判断最优的划分点
    - 注意：与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性，例如在父结点上使用了"密度<=0.381" ，不会禁止在子结点上使用"密度<=0.294"

- 缺失值处理

  - 问题

    - 如何在属性值缺失的情况F进行划分属性选择?
    - 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分?

  - 根据无缺失值数据子集计算各属性的信息增益，选定划分属性后，如有数据集在该划分属性上存在缺失，则按照属性的分布按比例划入该属性的多个取值中

    ![image-20200629155432452](https://i.loli.net/2020/06/29/pdP9HGauA85t2Ov.png)

    ![image-20200629155456568](https://i.loli.net/2020/06/29/7iXQryMF6s4Ajp1.png)

    ![image-20200629155516341](https://i.loli.net/2020/06/29/YAgirzTw4cmFWo9.png)

## 多变量决策树

- d个属性描述的样本可以视为对应d维空间中的一个数据点，决策树形成的分类边界是由若干与坐标轴平行的分段组成的，这种分类边界使得学习结果有较好的可解释性

  - 平行边界的缺陷：在复杂的决策树中，由于要进行大量的属性测试，预测时间开销会很大

  - 解决方案：多变量决策树（斜划分）

    ![image-20200629160800504](https://i.loli.net/2020/06/29/LcqDBFvmTJ8R7U6.png)

## 补充内容

- 剪枝主要能够影响泛化性能
- 多变量决策树算法
  - OC1
  - 引入线性分类器学习的最小二乘法
  - 引入神经网络：感知机树
- 增量学习
  - ID4
  - ID5R
  - ITI

## 课后作业

[决策树代码实现请参考](https://zhuanlan.zhihu.com/p/20794583)

[第四章作业](https://blog.csdn.net/icefire_tyh/article/details/52082054)





