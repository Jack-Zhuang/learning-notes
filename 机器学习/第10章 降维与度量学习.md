# 第10章 降维与度量学习

## k近邻学习

### 分类

- 根据离测试样本最近的k个样本的标签进行投票，出现最多的类别标记作为预测结果

### 回归

- 根据离测试样本最近的k个样本的值进行平均或加权投票，得到的结果作为预测结果

### 特点

- 无训练过程
- k值的确定对预测结果有极大的影响
- 泛化错误率最高不超过贝叶斯最优分类器错误率的两倍

## 低维嵌入

### 多维缩放

- 要求原始空间中样本之间的距离在低维空间中得以保持
- ![image-20201109121033552](https://i.loli.net/2020/11/09/imLxo9HasKqC4Uu.png)

## 主成分分析（principal component analysis）

- ![image-20201109125526462](https://i.loli.net/2020/11/09/cGYarQuy7otMUdz.png)

## 核化线性降维

### 解决问题

- 高维数据需要非线性映射才能找到恰当的低维嵌入，否则会丢失低维结构

## 流形学习

### 等度量映射（Isometric mapping）

- 高维的直线距离在低维嵌入流形上不可达，只能走测地线距离
- 测地线距离的计算可以用近邻点的连接关系，将问题转变为求近邻连接图上两点之间的最短路径问题（可用Dijkstra算法和Floyd算法求解）

![](https://i.loli.net/2020/11/12/3pw2PKtBHQgM9z1.png)

#### 新样本的处理方法

- 将训练样本的高维空间坐标作为输入，低维空间坐标作为输出，训练一个回归学习期来对新样本的低维坐标进行预测

#### 近邻图构建的问题

- 近邻范围过大，则过远的点会被误认为是近邻
- 近邻范围过小，则可能出现某些区域与其他区域不存在连接

### 局部线性嵌入（local linear embedding）

#### 与Isomap的区别

- Isomap保持近邻样本之间的距离
- 局部线性嵌入保留近邻样本之间的线性关系

## 度量学习

### 目的

- 学习一个合适的度量距离

### 近邻成分分析（Neighborhood Component Analysis，简称NCA）

- 求取一个（半）正定对称矩阵M作为度量矩阵，后采用随机梯度下降等方法，优化求取度量矩阵