第5章 神经网络

## 神经元模型

- 神经元模型示意图

  ![image-20200707145858080](https://i.loli.net/2020/07/07/foyBxVYCIWZ6GQA.png)

## 感知机（perceptron）与多层网络

### 组成

- 两层神经元

![image-20200721103428495](https://i.loli.net/2020/07/21/fEK69YJAuriknWb.png)

- 权重和阈值可通过学习得到，阈值可看作是一个固定输入为-1的哑结点

- 感知机的权重调整如下，其中η为学习率

  ![image-20200707175532202](https://i.loli.net/2020/07/07/SOpGPAbQr4o76uX.png)

### 参数

- （d+l+1）*q+l个，其中d是输入层神经元数，l是输出层神经元数，q是隐层神经元数

### 前馈神经网络

- 特点：每层神经元与下层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，在网络中信号可以向后传，但是网络拓扑结构中不存在环或回路
- 层
  - 输入层
    - 仅接受输入，不进行函数处理，不计算在层数内，因为不具备任何功能神经元
  - 隐藏层
    - 输出层与输入层之间的功能神经元，有激活函数
  - 输出层
    - 有激活函数的功能神经元

## 误差逆传播算法

### BP算法（以均方误差为例）

- ![image-20200721220054644](https://i.loli.net/2020/07/21/TZ9rJvK27bplIBY.png)
  
- 误差：
  
  ![image-20200721112842417](https://i.loli.net/2020/07/21/NxjmpAtYic71K45.png)
  
- 基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整

  ![image-20200721110355369](https://i.loli.net/2020/07/21/5FWT6dcvOrqj9a1.png)

  ![image-20200721113024707](https://i.loli.net/2020/07/21/V6MGZhK7z8Tt3Qv.png)

  其中

  ![image-20200721113605540](https://i.loli.net/2020/07/21/3DzNxrPT4nYE7KG.png)

  ![image-20200721115602501](https://i.loli.net/2020/07/21/Sa6O49Q8sM5cVlD.png)![image-20200721125449102](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200721125449102.png)

  ![image-20200721132116030](https://i.loli.net/2020/07/21/SQibCyBImo7ELJf.png)

###  **BP算法的目标**

- **最小化训练集D上的累积误差**，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现"抵消"现象

### 更新频率

- 标准BP算法
  - 需要更多次数的迭代，但在累积误差下降到一定程度时，此算法能更快得到较好的解
- 累积BP算法
  - 直接针对累积误差最小化，所以更新频率低

### 针对过拟合

- 早停（early stop）：若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值
- 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，使得训练过程会偏好比较小的连接权和阈值，使网络输出更加光滑，从而缓解过拟合

## 全局最小与局部极小

- 全局最小：是指参数空间中所有点的误差函数值均不小于该点的误差函数值
- 局部极小：是参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值
  - 参数空间内梯度为零的点
- 问题：存在多个局部极小时，参数寻优容易陷入局部极小
  - 解决策略
    - 从多个不同的初始值开始寻优，取其中误差最小的解作为最终参数
    - 使用模拟退火，在每一步都以一定的概率接受比当前解更差的结果，从而有助于"跳出"局部极小。在每步迭代过程中，接受"次优解"的概率要随着时间的推移而逐渐降低，从而保证算法稳定
    - 使用随机梯度下降，即便陷入局部极小点?，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索.

## 其他常见神经网络

### RBF（Radical Basis Function，径向基函数）网络

- 隐层神经元的激活函数为径向基函数，输出层则是对隐层神经元输出的线性组合

  ![image-20200727115040387](https://i.loli.net/2020/07/27/rZKSXkl2EbFU6tm.png)

  其中w是权重，c是神经元对应的中心，ρ是径向基函数（常用的有高斯径向基）

- 训练过程
  - 以随机采样、聚类等方法确定神经元中心
  - 利用BP算法确定w等参数

### ART（Adaptive Resonance Theory，自适应谐振理论）网络

- 网络组成
  - 比较层
    - 负责接收输入样本，并将其传递给识别层神经元
  - 识别层
    - 每个神经元对应一个模式类，神经元数目可以在训练过程中动态增长以增加新的模式类，每个神经元有一个代表向量
  - 识别阈值
    - 若输入向量与获胜神经元对应的代表向量之间的相似度大于识别阈值，则该样本归为代表向量所属类别，同时更新连接权
  - 重置模块
    - 若输入向量与获胜神经元对应的代表向量之间的相似度小于识别阈值，重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量
- ART可缓解“可塑性-稳定性窘境”
  - 可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆
  - 结论：ART网络具有可进行增量学习或在线学习的优点

### SOM（Self-Organizing Map ，自组织映射)网络

- 功能
  - 是一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维空间（通常是二维空间），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。
- 训练过程
  - 在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元 (best matching unit). 然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小，这个过程不断迭代，直至收敛.

### 级联相关网络

- 功能
  - 将网络结构也作为学习的目标之一
- 级联相关网络有两个主要成分"级联"和"相关" 
  - 级联是指建立层次连接的层级结构
  - 相关是指通过最大化新神经元的输出与网络误差之间的相关性(correlation)来训练相关的参数.
  - 在开始训练时，网络只有输入层和输出层，处于最小拓扑结构；随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构. 当新的隐层神经元加入时，其输入端连接权值是冻结固定的。

### Elman网络

- 是一种递归神经网络

![image-20200727173312428](https://i.loli.net/2020/07/27/z4VJ3uDaKmtiCeM.png)

### Boltzmann机

- 是一种基于能量的模型

- 神经元分为两层

  - 显层
    - 表示数据的输入与输出
  - 隐层
    - 数据的内在表达

- Boltzmann 机的训练过程就是将每个训练样本视为一个状态向量，使其出现的概率尽可能大.标准的Boltzmann 机是一个全连接图，训练网络的复杂度很高，这使其难以用于解决现实任务. 现实中常采用受限Boltzmann机(Restricted Boltzmann Machine，简称RBM)

  ![image-20200727175830484](https://i.loli.net/2020/07/27/fOQEtU2cIPykF43.png)

  ![image-20200727175859720](https://i.loli.net/2020/07/27/uVYDlhy1IwqnHQ4.png)

### 深度学习

- 多隐层堆叠
- 无监督逐层训练

## 课后作业

