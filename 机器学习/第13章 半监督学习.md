# 第13章 半监督学习

## 未标记样本

### 主动学习

- 用尽可能少的“查询”来获得尽可能好的性能，希望每一次都能选中能够有效改善模型性能的未标记样本，并将该样本提供给“专家”进行标记，从而降低标记成本，同时提高模型性能

### 半监督学习

- 整个过程完全没有与外界的交互，自动的利用未标记样本来提升学习性能
- 聚类假设与流形假设
  - 聚类假设：同一个簇的样本属于同一个类别
  - 流形假设：邻近的样本拥有相似的输出值，邻近程度以相似程度来刻画
- 本质是“相似的样本拥有相似的输出”

#### 半监督学习分类

- 纯半监督学习：假定训练数据中的未标记样本并非待预测数据
- 直推学习（transductive learning)：假定训练数据中的未标记样本是待预测数据
- ![image-20201123113100490](https://i.loli.net/2020/11/23/U5VDZcXkgGKSCit.png)

## 生成式方法

### 假设

- 假设所有数据(无论是否有标记)都是由同一个潜在的模型" 生成"的

### 高斯混合模型的极大似然估计法求解

- ![image-20201123120802879](https://i.loli.net/2020/11/23/OLP8kramoK3gC76.png)
- 参数求解可用EM算法
- ![image-20201123121633371](https://i.loli.net/2020/11/23/yZPJxAiwgQ9IunR.png)



## 半监督SVM

### TSVM(Transductive Support Vector Machine)

- 针对二分类问题，对未标记样本进行标记指派，即尝试将每个未标记样本分别作为正例或反例3 然后在所有这些结果中， 寻求一个在所有样本(包括有标记样本和进行了标记指派的未标记样本)上间隔最大化的划分超平面. 一旦划分超平面得以确定，未标记样本的最终标记指派就是其预测结果，是一个穷举过程，仅当未标记样本很少时才可直接求解
- ![image-20201128165635089](https://i.loli.net/2020/11/28/toYRbUd6G1KMwLS.png)

## 图半监督学习

![image-20201129121439113](https://i.loli.net/2020/11/29/h4Ga8YHJ29FOqTN.png)

- 缺点：涉及的矩阵规模是样本数的平方，难以处理大规模数据，难以处理新数据（两个方案：重新进行标记传播，或引入其他预测机制）

## 基于分歧的方法

- 多学习器
- 针对多视图数据（具有多个属性集）
- 协同训练
- ![image-20201129122919427](https://i.loli.net/2020/11/29/iO71Id4pGW9ezLF.png)
- 不同视图，不同算法、不同数据采样、不同参数设置等都是产生差异的渠道

## 半监督聚类

![image-20201129123629528](https://i.loli.net/2020/11/29/FEBAQN7twpnWXae.png)

![image-20201129123845428](https://i.loli.net/2020/11/29/xTt9eRjJB3271XM.png)