# 第11章 特征选择与稀疏学习

## 子集搜索与评价

### 遍历子集的问题

- 遍历不可行，组合数爆炸
- 思路：基于当前候选子集的评价产生下一个候选子集
  - 问题：
    - 如何根据评价获取下一个候选子集
    - 如何评价候选子集的好坏

### 子集搜索问题（贪心策略）

- 前向策略
  - 第一步选取最优特征；第二步以第一步为基础增加一个特征，并从中选择表现最优的作为下一步的基础；直到第n+1步组合结果不优于第n步的最优结果，此时输出第n步的最优结果
- 后向策略
  - 与前向策略相反，逐个去除特征

## 子集评价问题

- 信息增益法

## 过滤式选择

### 定义

- 先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关

### Relief方法

- 思路：找出一个相关统计量来度量每个特征的重要性，我们认为同类样本中，如果选定属性使得最邻近的同类样本的距离小于最邻近的异类样本的距离，则该属性对于分类起正面作用，可增大该属性对应的相关统计量

## 包裹式选择

### 定义

- 直接把最终将要使用的学习器的性能作为特征于集的评价准则，针对给定学习器进行优化，性能一般比过滤式特征选择更好，但是需要多次训练学习器，开销远大于过滤式特征选择

### LVW（Las Vegas Wrapper）方法

- 特点：运行时间较长，在特征数很多或停止条件T过大的情况下可能给不出解

![image-20201116103227337](https://i.loli.net/2020/11/16/VkxOvTr4iABUyah.png)

## 嵌入式选择与L1正则化

### 特点

- 特征选择过程与学习器训练过程融为一体

### L1正则化（LASSSO）

- L1范数正则化除了降低过拟合风险外，还可以得到权重的稀疏解，而权重的稀疏解意味着一种嵌入式的特征选择，通过PGD方法可以求解

## 稀疏表示与字典学习

- 扩充维度，使得非稀疏的数据转变为稀疏的数据，从而更容易达到高维线性可分，从而提高分类性能，这个过程称为字典学习，也称为稀疏编码，求解方法可参照LASSO

## 压缩感知

### 感知测量

- 关注如何对原始信号进行处理以获得稀疏样本表示

### 重构恢复

- 关注的是如何基于稀疏性从少量观测中恢复原信号
- 矩阵补全技术，限定等距性理论